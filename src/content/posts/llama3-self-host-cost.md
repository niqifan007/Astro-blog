---
title: Llama-3 8B-Instructè‡ªæˆ‘æ‰˜ç®¡çš„æˆæœ¬(è½¬è½½)
published: 2024-06-15
description: 'å¯¹llama-3 8B-Instructè‡ªæˆ‘æ‰˜ç®¡çš„æˆæœ¬è¿›è¡Œåˆ†æ'
image: 'https://image.dooo.ng/c/2024/06/16/666dc681a25a9.webp'
tags: [LLM, self-host]
category: 'Guides'
draft: false 
lang: ''
---
## Cost Of Self Hosting Llama-3 8B-Instruct  
è‡ªæˆ‘æ‰˜ç®¡çš„æˆæœ¬ Llama-3 8B-Instruct

Sid Premkumar

,Â Sid Premkumarã€Thu Jun 13 2024Â 2024 å¹´ 6 æœˆ 13 æ—¥æ˜ŸæœŸå››â€¢[llama-3Â éª†é©¼-3](https://blog.lytix.co/tags/llama-3)

![Cover Image](https://blog.lytix.co/posts/self-hosting-llama-3/cover.webp)

## How much does it cost to self host a LLM?  
è‡ªåŠ©æ‰˜ç®¡ LLM çš„è´¹ç”¨æ˜¯å¤šå°‘ï¼Ÿ[](https://blog.lytix.co/posts/self-hosting-llama-3#how-much-does-it-cost-to-self-host-a-llm)

#### âš¡ï¸ TLDR: Assuming 100% utilization of your model `Llama-3 8B-Instruct` model costs about $17 dollars per 1M tokens when self hosting with EKS, vs ChatGPT with the same workload can offer $1 per 1M tokens. Choosing to self host the hardware can make the cost <$0.01 per 1M token that takes ~5.5 years to break even.  
âš¡ï¸ TLDRï¼šå‡è®¾æ‚¨çš„ `Llama-3 8B-Instruct` æ¨¡å‹ 100% ä½¿ç”¨ EKS è‡ªæ‰˜ç®¡ï¼Œåˆ™æ¯ 100 ä¸‡ä¸ªä»£å¸çš„æˆæœ¬çº¦ä¸º 17 ç¾å…ƒï¼Œè€Œ ChatGPT åœ¨ç›¸åŒå·¥ä½œé‡ä¸‹æ¯ 100 ä¸‡ä¸ªä»£å¸çš„æˆæœ¬ä»…ä¸º 1 ç¾å…ƒã€‚é€‰æ‹©è‡ªè¡Œæ‰˜ç®¡ç¡¬ä»¶å¯ä»¥ä½¿æ¯ 100 ä¸‡ä¸ªä»£å¸çš„æˆæœ¬ä½äº 0.01 ç¾å…ƒï¼Œè¿™éœ€è¦ ~5.5 å¹´æ‰èƒ½å®ç°æ”¶æ”¯å¹³è¡¡ã€‚[](https://blog.lytix.co/posts/self-hosting-llama-3#%EF%B8%8F-tldr-assuming-100-utilization-of-your-model-llama-3-8b-instruct-model-costs-about-17-dollars-per-1m-tokens-when-self-hosting-with-eks-vs-chatgpt-with-the-same-workload-can-offer-1-per-1m-tokens-choosing-to-self-host-the-hardware-can-make-the-cost-001-per-1m-token-that-takes-55-years-to-break-even)

A question we often get is how do you scale with ChatGPT. One thing we wanted to try is to determine the cost of self hosting an open model such as Llama-3.  
æˆ‘ä»¬ç»å¸¸é‡åˆ°çš„ä¸€ä¸ªé—®é¢˜æ˜¯å¦‚ä½•æ‰©å±• ChatGPTã€‚æˆ‘ä»¬æƒ³åšçš„ä¸€ä»¶äº‹å°±æ˜¯ç¡®å®šè‡ªè¡Œæ‰˜ç®¡ Llama-3 ç­‰å¼€æ”¾æ¨¡å‹çš„æˆæœ¬ã€‚

  

##### Determining the best hardware  
ç¡®å®šæœ€ä½³ç¡¬ä»¶[](https://blog.lytix.co/posts/self-hosting-llama-3#determining-the-best-hardware)

_Context_: All tests were run on a EKS cluster. Each test was allocated the resources of the entire node, nothing else was running on that node other than system pods (prometheus, nvidia-daemon, etc.)  
èƒŒæ™¯ï¼šæ‰€æœ‰æµ‹è¯•å‡åœ¨ EKS é›†ç¾¤ä¸Šè¿è¡Œã€‚æ¯ä¸ªæµ‹è¯•éƒ½åˆ†é…äº†æ•´ä¸ªèŠ‚ç‚¹çš„èµ„æºï¼Œé™¤äº†ç³»ç»Ÿ podï¼ˆprometheusã€nvidia-daemon ç­‰ï¼‰å¤–ï¼Œè¯¥èŠ‚ç‚¹ä¸Šæ²¡æœ‰è¿è¡Œå…¶ä»–ä»»ä½•ä¸œè¥¿ã€‚

We first wanted to start small, can we run it on a single Nvidia Tesla T4 GPU? I started with AWSâ€™s `g4dn.2xlarge` instance. Its specs were as follows ([source (opens in a new tab)](https://aws.amazon.com/ec2/instance-types/g4/)):  
æˆ‘ä»¬é¦–å…ˆæƒ³ä»å°è§„æ¨¡å¼€å§‹ï¼Œèƒ½å¦åœ¨å•ä¸ª Nvidia Tesla T4 GPU ä¸Šè¿è¡Œï¼Ÿæˆ‘ä» AWS çš„ `g4dn.2xlarge` å®ä¾‹å¼€å§‹ã€‚å…¶è§„æ ¼å¦‚ä¸‹ï¼ˆæ¥æºï¼‰ï¼š

-   1 NVidia Tesla T4
-   32GB Memory.Â 32GB å†…å­˜
-   8 vCPUsÂ 8 vCPU

The short answer is that running either the 8B param or the 70B param version of Llama 3 did not work on this hardware. We initially tried the 70B param version of Llama 3 and constantly ran into OOM issues. We then sobered up and tried the 8B param version. Although this time we was able to get a response, generating the response took what felt like ~10 minutes. I checked `nvidia-smi` and the single GPU was being used, it just wasnâ€™t enough.  
ç®€è€Œè¨€ä¹‹ï¼Œåœ¨è¿™ç§ç¡¬ä»¶ä¸Šè¿è¡Œ 8B å‚æ•°æˆ– 70B å‚æ•°ç‰ˆæœ¬çš„ Llama 3 éƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚æˆ‘ä»¬æœ€åˆå°è¯•äº† 70B å‚æ•°ç‰ˆæœ¬çš„ Llama 3ï¼Œä½†ç»å¸¸å‡ºç° OOM é—®é¢˜ã€‚åæ¥æˆ‘ä»¬å†·é™ä¸‹æ¥ï¼Œå°è¯•äº† 8B å‚æ•°ç‰ˆæœ¬ã€‚è™½ç„¶è¿™æ¬¡æˆ‘ä»¬èƒ½å¾—åˆ°å“åº”ï¼Œä½†ç”Ÿæˆå“åº”å´èŠ±äº†å¤§çº¦ 10 åˆ†é’Ÿã€‚æˆ‘æ£€æŸ¥äº† `nvidia-smi` ï¼Œå‘ç°å• GPU ä¹Ÿåœ¨ä½¿ç”¨ï¼Œåªæ˜¯ä¸å¤Ÿç”¨ã€‚

For context 8B vs 70B refers to the parameters in the model and generally translate to performance of the model. The advantage of the 8B param is its small size and resource footprint.  
å°±ä¸Šä¸‹æ–‡è€Œè¨€ï¼Œ8B ä¸ 70B æŒ‡çš„æ˜¯æ¨¡å‹ä¸­çš„å‚æ•°ï¼Œä¸€èˆ¬è½¬åŒ–ä¸ºæ¨¡å‹çš„æ€§èƒ½ã€‚8B å‚æ•°çš„ä¼˜ç‚¹æ˜¯ä½“ç§¯å°ã€å ç”¨èµ„æºå°‘ã€‚

![CUDA OOM](https://blog.lytix.co/posts/self-hosting-llama-3/cuda-oom.png)

So I decided to switch to the `g4dn.16xlarge` instance type. The specs of that were as follows ([source (opens in a new tab)](https://aws.amazon.com/ec2/instance-types/g4/)):  
å› æ­¤ï¼Œæˆ‘å†³å®šæ”¹ç”¨ `g4dn.16xlarge` å®ä¾‹ç±»å‹ã€‚å…¶è§„æ ¼å¦‚ä¸‹ï¼ˆæ¥æºï¼‰ï¼š

-   4 NVidia Tesla T4s
-   192gb memoryÂ 192GB å†…å­˜
-   48 vCPUsÂ 48 ä¸ª vCPU

##### Initial ImplementationÂ åˆæ­¥å®æ–½[](https://blog.lytix.co/posts/self-hosting-llama-3#initial-implementation)

My initial implementation involved trying to copy and paste the code present on the llama-3 hugging face ([see (opens in a new tab)](https://huggingface.co/meta-llama/Meta-Llama-3-8B))  
æˆ‘åœ¨æœ€åˆçš„å®æ–½è¿‡ç¨‹ä¸­ï¼Œå°è¯•å¤åˆ¶å’Œç²˜è´´å­˜åœ¨äº llama-3 æŠ±æŠ±è„¸ä¸Šçš„ä»£ç ï¼ˆè§ï¼‰

These results seemed a lot more promising as the response time lowered from ~10m to sub 10 seconds consistently.  
è¿™äº›ç»“æœä¼¼ä¹æ›´æœ‰å¸Œæœ›ï¼Œå› ä¸ºå“åº”æ—¶é—´ä»å¤§çº¦ 10 ç±³æŒç»­ç¼©çŸ­åˆ° 10 ç§’ä»¥ä¸‹ã€‚

![Initial Implementation Results](https://blog.lytix.co/posts/self-hosting-llama-3/inital-implementation-results.png)

I was able to get a response in a much more reasonable 5-7 seconds. At this point I wanted to start calculating the cost of this request.  
æˆ‘åªç”¨äº† 5-7 ç§’å°±å¾—åˆ°äº†åˆç†å¾—å¤šçš„å›å¤ã€‚æ­¤æ—¶ï¼Œæˆ‘æƒ³å¼€å§‹è®¡ç®—è¿™ä¸ªè¯·æ±‚çš„æˆæœ¬ã€‚

`g5dn.12xlarge` costs $3.912 per hour on demand ([see (opens in a new tab)](https://aws.amazon.com/ec2/instance-types/g5/)).  
`g5dn.12xlarge` çš„éœ€æ±‚æˆæœ¬ä¸ºæ¯å°æ—¶ 3.912 ç¾å…ƒï¼ˆè§ï¼‰ã€‚

If we assume a full month of use, that costs:  
å¦‚æœå‡å®šä½¿ç”¨ä¸€ä¸ªæœˆï¼Œè´¹ç”¨ä¸º

I had trouble with getting the token count in hugging face so ended up using [llama-tokenizer-js (opens in a new tab)](https://belladoreai.github.io/llama-tokenizer-js/example-demo/build/) to get an approximation of tokens used.  
æˆ‘åœ¨è·å– "æ‹¥æŠ±è„¸ "ä¸­çš„ä»¤ç‰Œæ•°æ—¶é‡åˆ°äº†éº»çƒ¦ï¼Œå› æ­¤æœ€ç»ˆä½¿ç”¨ llama-tokenizer-js æ¥è·å–ä»¤ç‰Œä½¿ç”¨é‡çš„è¿‘ä¼¼å€¼ã€‚

![Llama-3 Tokenizer JS](https://blog.lytix.co/posts/self-hosting-llama-3/llama-3-tokenizer-js.png)

__Note: This ended up being incorrect way of calculating the tokens used.  
æ³¨ï¼šè¿™æ˜¯è®¡ç®—æ‰€ç”¨ä»£å¸çš„ä¸æ­£ç¡®æ–¹æ³•ã€‚__

If we look at this result, we assume we used 39 tokens over ~6 seconds. Assuming Im processing tokens 24/7, extrapolating that we get:  
å¦‚æœæˆ‘ä»¬çœ‹ä¸€ä¸‹è¿™ä¸ªç»“æœï¼Œå‡è®¾æˆ‘ä»¬åœ¨ ~6 ç§’å†…ä½¿ç”¨äº† 39 ä¸ªä»¤ç‰Œã€‚å‡å®š Im 24/7 å¤„ç†ä»¤ç‰Œï¼Œæ¨æ–­å‡ºï¼š

With 6.5 tokens per second, over the month I can process:  
ä»¥æ¯ç§’ 6.5 ä¸ªä»¤ç‰Œè®¡ç®—ï¼Œä¸€ä¸ªæœˆä¸‹æ¥æˆ‘å¯ä»¥å¤„ç† 6.5 ä¸ªä»¤ç‰Œï¼š

With 16,848,000 tokens per month, every million tokens costs:  
æ¯æœˆæœ‰ 1684.8 ä¸‡ä¸ªä»£å¸ï¼Œæ¯ä¸€ç™¾ä¸‡ä¸ªä»£å¸çš„æˆæœ¬ï¼š

ğŸ˜µYikes. It is not looking good. Lets compare this to what ChatGPT 3.5 can get us.  
ğŸ˜µå‘€ã€‚çœ‹èµ·æ¥ä¸å¦™ã€‚è®©æˆ‘ä»¬æŠŠå®ƒä¸ ChatGPT 3.5 èƒ½å¸¦ç»™æˆ‘ä»¬çš„ä¸œè¥¿è¿›è¡Œæ¯”è¾ƒã€‚

Looking at their [pricing (opens in a new tab)](https://openai.com/api/pricing/), ChatGPT 3.5 Turbo charges $0.5 per 1M input token, and $1.5 per 1M output token. For sake of simplicity, assuming an average input:output ratio, that means per 1M tokens they charge $1 and thats the number to beat. Far from my $167.17.  
ä»å®šä»·æ¥çœ‹ï¼ŒChatGPT 3.5 Turbo æ¯ 100 ä¸‡ä¸ªè¾“å…¥ä»£å¸æ”¶å– 0.5 ç¾å…ƒï¼Œæ¯ 100 ä¸‡ä¸ªè¾“å‡ºä»£å¸æ”¶å– 1.5 ç¾å…ƒã€‚ä¸ºç®€å•èµ·è§ï¼Œå‡è®¾å¹³å‡è¾“å…¥è¾“å‡ºæ¯”ä¸º 1:1ï¼Œè¿™æ„å‘³ç€æ¯ 100 ä¸‡ä¸ªä»£å¸æ”¶è´¹ 1 ç¾å…ƒï¼Œè¿™å°±æ˜¯è¦å‡»è´¥çš„æ•°å­—ã€‚ä¸æˆ‘çš„ 167.17 ç¾å…ƒç›¸å·®ç”šè¿œã€‚

##### Realization Something Was Wrong  
æ„è¯†åˆ°å‡ºäº†é—®é¢˜[](https://blog.lytix.co/posts/self-hosting-llama-3#realization-something-was-wrong)

At this point I felt I was doing something wrong. Llama 3 with 8B params is hard to run, but I didnâ€™t feel it should be this hard, especially considering I have 4 GPUs available ğŸ¤”.  
æ­¤æ—¶ï¼Œæˆ‘è§‰å¾—è‡ªå·±åšé”™äº†ä»€ä¹ˆã€‚å¸¦æœ‰ 8B å‚æ•°çš„ã€ŠLlama 3ã€‹å¾ˆéš¾è¿è¡Œï¼Œä½†æˆ‘è§‰å¾—ä¸åº”è¯¥è¿™ä¹ˆéš¾ï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°æˆ‘æœ‰ 4 ä¸ª GPU å¯ç”¨ğŸ¤”ã€‚

I decided to try to use vLLM to host an API server instead of attempting to do it myself via hugging face libraries.  
æˆ‘å†³å®šå°è¯•ä½¿ç”¨ vLLM æ¥æ‰˜ç®¡ä¸€ä¸ª API æœåŠ¡å™¨ï¼Œè€Œä¸æ˜¯è¯•å›¾è‡ªå·±é€šè¿‡æŠ±è„¸åº“æ¥åšè¿™ä»¶äº‹ã€‚

This was dead simple and just involved me installing `ray` and `vllm` via `pip3` and then changing my docker entry point to:  
è¿™éå¸¸ç®€å•ï¼Œæˆ‘åªéœ€é€šè¿‡ `pip3` å®‰è£… `ray` å’Œ `vllm` ï¼Œç„¶åå°† docker å…¥å£ç‚¹æ›´æ”¹ä¸º

Specifically noting that I call `â€”tensor-parallel-size 4` to enforce that we use all 4 GPUs.  
ç‰¹åˆ«è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘è°ƒç”¨ `â€”tensor-parallel-size 4` æ¥å¼ºåˆ¶ä½¿ç”¨å…¨éƒ¨ 4 ä¸ª GPUã€‚

Using this got _significantly_ better results.  
è¿™æ ·åšçš„æ•ˆæœæ˜æ˜¾æ›´å¥½ã€‚

![Llama-3 Tokenizer JS](https://blog.lytix.co/posts/self-hosting-llama-3/significantly-better-results.png)

In this example you can see the query took 2044ms to complete. This was also the time I realized my previous method of calculating token usage was incorrect. vLLM returned the tokens used which was perfect.  
åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°æŸ¥è¯¢è€—æ—¶ 2044 æ¯«ç§’æ‰å®Œæˆã€‚è¿™ä¹Ÿæ˜¯æˆ‘æ„è¯†åˆ°ä¹‹å‰è®¡ç®—ä»¤ç‰Œä½¿ç”¨é‡çš„æ–¹æ³•ä¸æ­£ç¡®çš„æ—¶å€™ã€‚

Going back to cost, now we can calculate tokens per second:  
å›åˆ°æˆæœ¬ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ç§’çš„ä»£å¸ï¼š

Again assuming I produce tokens 24/7, this means that I can ingest a total of:  
å†æ¬¡å‡è®¾æˆ‘å…¨å¤©å€™ç”Ÿäº§ä»£å¸ï¼Œè¿™æ„å‘³ç€æˆ‘æ€»å…±å¯ä»¥æ‘„å…¥ï¼š

Which means the cost per 1 million tokens would cost me:  
ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ 100 ä¸‡ä¸ªä»£å¸çš„æˆæœ¬å°±æ˜¯æˆ‘çš„æˆæœ¬ï¼š

Unfortunately this is still not below the value that ChatGPT offers, youâ€™d lose about $17 a day ğŸ˜­  
ä¸å¹¸çš„æ˜¯ï¼Œè¿™ä»ç„¶ä¸ä½äº ChatGPT æä¾›çš„ä»·å€¼ï¼Œæ‚¨æ¯å¤©å°†æŸå¤±çº¦ 17 ç¾å…ƒ ğŸ˜­

##### An Unconventional Approach  
éå¸¸è§„æ–¹æ³•[](https://blog.lytix.co/posts/self-hosting-llama-3#an-unconventional-approach)

Instead of using AWS another approach involves self hosting the hardware as well. Even after factoring in energy, this does dramatically lower the price.  
å¦ä¸€ç§æ–¹æ³•æ˜¯ä¸ä½¿ç”¨ AWSï¼Œè€Œæ˜¯è‡ªè¡Œæ‰˜ç®¡ç¡¬ä»¶ã€‚å³ä½¿è€ƒè™‘åˆ°èƒ½æºå› ç´ ï¼Œè¿™ä¹Ÿèƒ½å¤§å¹…é™ä½ä»·æ ¼ã€‚

Assuming we want to mirror our setup in AWS, weâ€™d need 4xNVidia Tesla T4s. You can buy them for about $700 dollars on eBay  
å‡è®¾æˆ‘ä»¬æƒ³åœ¨ AWS ä¸­é•œåƒæˆ‘ä»¬çš„è®¾ç½®ï¼Œæˆ‘ä»¬éœ€è¦ 4xNVidia Tesla T4ã€‚ä½ å¯ä»¥åœ¨ eBay ä¸Šä»¥çº¦ 700 ç¾å…ƒçš„ä»·æ ¼ä¹°åˆ°å®ƒä»¬

![Ebay Listing](https://blog.lytix.co/posts/self-hosting-llama-3/ebay-listing.png)

Add in $1,000 to setup the rest of the rig and you have a final price of around:  
å†åŠ ä¸Šå®‰è£…å…¶ä»–è®¾å¤‡çš„ 1,000 ç¾å…ƒï¼Œæœ€ç»ˆä»·æ ¼çº¦ä¸º 1,000 ç¾å…ƒï¼š

If we calculate energy for this, we get about ~$50 bucks. I determined power consumption by 70W per GPU + 20W overhead:  
å¦‚æœæŒ‰æ­¤è®¡ç®—èƒ½è€—ï¼Œå¤§çº¦éœ€è¦ 50 ç¾å…ƒã€‚æˆ‘æŒ‰ç…§æ¯ä¸ª GPU 70W + 20W çš„å¼€é”€æ¥è®¡ç®—åŠŸè€—ï¼š

![Energy Cost](https://blog.lytix.co/posts/self-hosting-llama-3/energy-cost.png)

After factoring in the ~$3,800 fixed cost, you have a monthly cost of ~$50 bucks, lets round up to ~$100 to factor any misc items we might have missed.  
æ‰£é™¤çº¦ 3 800 ç¾å…ƒçš„å›ºå®šæˆæœ¬åï¼Œæ¯æœˆæˆæœ¬çº¦ä¸º 50 ç¾å…ƒï¼Œå››èˆäº”å…¥åçº¦ä¸º 100 ç¾å…ƒï¼Œä»¥è€ƒè™‘æˆ‘ä»¬å¯èƒ½é—æ¼çš„ä»»ä½•æ‚é¡¹ã€‚

Re-calculating our cost per 1M tokens now:  
ç°åœ¨é‡æ–°è®¡ç®—æ¯ 100 ä¸‡ä¸ªä»£å¸çš„æˆæœ¬ï¼š

Which is **significantly** cheaper than our ChatGPT costs.  
è¿™æ¯”æˆ‘ä»¬çš„ ChatGPT è´¹ç”¨ä¾¿å®œå¾—å¤šã€‚

Trying to determine when youâ€™d break even, assuming you want to produce 157,075,200 tokens with ChatGPT, youâ€™re looking at a bill of:  
å‡å®šæ‚¨æƒ³é€šè¿‡ ChatGPT ç”Ÿäº§ 157,075,200 ä¸ªä»£å¸ï¼Œè¦ç¡®å®šä½•æ—¶æ‰èƒ½å®ç°æ”¶æ”¯å¹³è¡¡ï¼Œæ‚¨éœ€è¦æ”¯ä»˜çš„è´¦å•æ˜¯

You have a fixed cost of ~$100 a month, which results in a â€˜profitâ€™ of $57. To make up your initial server cost of $3,800, youâ€™d need about 66 months or 5.5 years to benefit from this approach.  
æ‚¨æ¯æœˆçš„å›ºå®šæˆæœ¬çº¦ä¸º 100 ç¾å…ƒï¼Œå› æ­¤ "åˆ©æ¶¦ "ä¸º 57 ç¾å…ƒã€‚è‹¥è¦å¼¥è¡¥åˆå§‹æœåŠ¡å™¨æˆæœ¬ 3,800 ç¾å…ƒï¼Œæ‚¨éœ€è¦å¤§çº¦ 66 ä¸ªæœˆæˆ– 5.5 å¹´çš„æ—¶é—´æ‰èƒ½ä»è¿™ç§æ–¹æ³•ä¸­è·ç›Šã€‚

Although this approach does come with negatives such as having to manage and scale your own hardware, it does seem to be possible to undercut the prices that ChatGPT offer by a significant amount in theory. In pracitice however, you'd have to evaluate how often you are utilizing your LLM, these hypotheticals all assume 100% utilization which is not realistic and would have to be tailord per use case.  
è™½ç„¶è¿™ç§æ–¹æ³•æœ‰ä¸€äº›ä¸åˆ©å› ç´ ï¼Œæ¯”å¦‚éœ€è¦ç®¡ç†å’Œæ‰©å±•è‡ªå·±çš„ç¡¬ä»¶ï¼Œä½†ä»ç†è®ºä¸Šè®²ï¼Œå®ƒç¡®å®æœ‰å¯èƒ½å¤§å¤§ä½äº ChatGPT æä¾›çš„ä»·æ ¼ã€‚ä½†åœ¨å®é™…æ“ä½œä¸­ï¼Œæ‚¨å¿…é¡»è¯„ä¼° LLM çš„ä½¿ç”¨é¢‘ç‡ï¼Œè¿™äº›å‡è®¾éƒ½æ˜¯å‡å®š 100% çš„ä½¿ç”¨ç‡ï¼Œä½†è¿™å¹¶ä¸ç°å®ï¼Œå¿…é¡»æ ¹æ®ä½¿ç”¨æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚

<small data-immersive-translate-walked="3d266d8d-f2d7-44f3-8fb5-3680b71e2b23" data-immersive-translate-paragraph="1"><time data-immersive-translate-walked="3d266d8d-f2d7-44f3-8fb5-3680b71e2b23">2024</time> Â© lytix.ai.</small>